Naive bayes classifier continues:
- Generative story for bernouli naive bayes
 - if head, flip each of V spam coins, One per word
 - if tails, flip each of V non-spam coins, One per word
 - We do not really generate emails this way -- but this is what we assume to be 
   the process how emails are generated

Generative multinomial naive bayes: 
- flip a weighted coin 
- if head, roll the spam die M times and record the count for each of the v sides 
- if tails roll te non spam die M times and record the count for each of the v sides

Multinomial vs bernouli:
- Bernouli:
 - X_i \in {0,1}
 - P(x|y) = \prod^v_{i=1} p(x_i|y)
 - Single occurance is equivalent to multiple occurance
- Multinomial:
 - x_i \in N_0
 - P(x|y) = p(y) \prod^v_{i=1} P(w_i|y)^{x_i} => w= word, x_i is count of how many times
 - Multiple occurances make a difference. we take count into consideration

MLE for NB with bernouli:
- Given  a set of N training emails, MLE of the parameters are:
 - P(y=1) = N_1/N where N_1 is the number of spams

- for each feature i, learn a bernouli model for each class:
 - P(x_i = 1| y = 1) = N_i|1 / N1, N_i|1 = # of spams the i-th word appear
 - P(x_i = 1| y = 0) = N_i|0 / N_0

MLE for naive bayes with Multi-nomial model
- MLE estimate for the ith word in the dictionary
P(w_i|y) = total # of word i in class y emails / total # of words on class y emails

- total number of parameters:
 - k(V-1)+(k-1)

Discrete and continuous Features:
- Naive  bayes canbe easily extended to handle features that are not binary-valued
- Discrete: x_i \in {1,2,...,k_i}
 - P(x_i = i| y) for j \in {1,2,...,k_j} - categorical distribution in place of bernouli
- Continuous:
 - Look at slides 

Problem with MLE:
- Suppose you picked up a new word "Mahalanobis" in your class and started using 
  it in your email x
- becauser the new word has never appeared before in training set
 - this will force the email to be non spam
 - solution is to remove this new word

Example bernouli:
- Given an unfair coin, we want to estimate \theta - the probability of head
- we toss n times and see n_1 heads
- MLE estimates: \theta = n_1/n

Beta destribution:
- p(\theta; \alpha,\beta) = 1/(B(\alpha,\beta))\theta^{\alpha-1}(1-\theta)^{\beta-1}
- we chose \alpha and \beta value

Posterior distribution of \theta
- p(\theta | D) = \frac{p(D|\theta)P(\theta)}{p(D)}
- derivation on slides
- p(\theta|D) = \frac{1}{B(n_1 + \alpha, n_0 + \beta)} \theat^{n_1+\alpha - 1}(1-\theta)^{n_0 + \beta -1}
- n_0 and n_1 are number of observed heads and tails
- \alpha and \beta are posterior

Maximum a-posterior MAP:
- p(y|x) = \integral p(y|x,\theta)... look at slides

MAP for bernouli:
- look at slides
- laplace estimation
- laplace smoothing

MAP estimation for Multi-nomial
- The conjugate prior formultinomial is called dirichlet distribution
- P(z=k) = frac{}{}

Laplace Smoothing for multinomial case:
- Look at slides

Summary:
- Generative
 - learn P(x|y) and p(y)
 - use bayes rule to compute P(y|x) for classification
 - Predict with argmaxP(y|x) or use decision theory

- Naive bayes assumes conditional independance
 - Greatly reduces the number of parameters to learn
___

Linear classification models: Perceptron
- Basic concept:
- The perceptron algorithm
- Perceptron loss/ hinge loss
- Subgradient decent
- Convergence proof of Perceptron
- Concept of Margin
- Voted and average Perceptron

linear classifier:
- w_1x_1 + w_2x_2 w_0 > 0 vs w_1x_1 + w_2x_2 w_0 < 0
- plot vector: w_1x_1 + w_2x_2 w_0 = 0
 - this is a perpendicular vector pointing to direction of positive
 - scale does not matter
 - we can scale it to any value

- LR learns:
 - P(y=1 | x) = 1/1+exp(-w^Tx)
 - which yields a linear decision boundry w^Tx = 0

- We will now look at a different paradigm

Binary Classification: General setup
- Given a set of training examplem (x_1,y_1),...,(x_n,y_n)
- each c_i \R^d, y \in {-1,1}
- linear function: w_0 + w_1x_1 + ... + w_dx_d
- look at slides


Loss function:
- J(w) 1/n \sum^n_{m=16}L(g(w,x_m),y_m)
 - where L(...) is the loss of g(...) given its true label y
-0/1 loss
 - L_{0/1}(g(...),y) = 1 if g(w,x) predicts wrong, 0 otherwise
 - this is not achievable:
  - piecewise, flat
  - non convex: NP_Hard to optimize
  - non-smooth: does not produce useful gradient since the surface of J_{0/1} p/w flat

- Perceptron Loss:
 - L_P(g(w,x),y) = max(0,yw^Tx)
 - if it is correct: (prediction is positive)
  - L_p = max(0,-yw^Tx) = 0
 - if it is incorrect:(prediction is negative)
  - L_p = max(0,yw^Tx) = yw^Tx
 - loss is a linear function of weight

J_P(w) = 1/n \sum^n_{m=1}max(0,-y_mw^Tx_m)
