Naive bayes classifier continues:
- Generative story for bernouli naive bayes
 - if head, flip each of V spam coins, One per word
 - if tails, flip each of V non-spam coins, One per word
 - We do not really generate emails this way -- but this is what we assume to be 
   the process how emails are generated

Generative multinomial naive bayes: 
- flip a weighted coin 
- if head, roll the spam die M times and record the count for each of the v sides 
- if tails roll te non spam die M times and record the count for each of the v sides

Multinomial vs bernouli:
- Bernouli:
 - X_i \in {0,1}
 - P(x|y) = \prod^v_{i=1} p(x_i|y)
 - Single occurance is equivalent to multiple occurance
- Multinomial:
 - x_i \in N_0
 - P(x|y) = p(y) \prod^v_{i=1} P(w_i|y)^{x_i} => w= word, x_i is count of how many times
 - Multiple occurances make a difference. we take count into consideration

MLE for NB with bernouli:
- Given  a set of N training emails, MLE of the parameters are:
 - P(y=1) = N_1/N where N_1 is the number of spams

- for each feature i, learn a bernouli model for each class:
 - P(x_i = 1| y = 1) = N_i|1 / N1, N_i|1 = # of spams the i-th word appear
 - P(x_i = 1| y = 0) = N_i|0 / N_0

MLE for naive bayes with Multi-nomial model
- MLE estimate for the ith word in the dictionary
P(w_i|y) = total # of word i in class y emails / total # of words on class y emails

- total number of parameters:
 - k(V-1)+(k-1)

Discrete and continuous Features:
- Naive  bayes canbe easily extended to handle features that are not binary-valued
- Discrete: x_i \in {1,2,...,k_i}
 - P(x_i = i| y) for j \in {1,2,...,k_j} - categorical distribution in place of bernouli
- Continuous:
 - Look at slides 

Problem with MLE:
- Suppose you picked up a new word "Mahalanobis" in your class and started using 
  it in your email x
- becauser the new word has never appeared before in training set
 - this will force the email to be non spam
 - solution is to remove this new word

Example bernouli:
- Given an unfair coin, we want to estimate \theta - the probability of head
- we toss n times and see n_1 heads
- MLE estimates: \theta = n_1/n

Beta destribution:
- p(\theta; \alpha,\beta) = 1/(B(\alpha,\beta))\theta^{\alpha-1}(1-\theta)^{\beta-1}
- we chose \alpha and \beta value

Posterior distribution of \theta
- p(\theta | D) = \frac{p(D|\theta)P(\theta)}{p(D)}
- derivation on slides
- p(\theta|D) = \frac{1}{B(n_1 + \alpha, n_0 + \beta)} \theat^{n_1+\alpha - 1}(1-\theta)^{n_0 + \beta -1}
- n_0 and n_1 are number of observed heads and tails
- \alpha and \beta are posterior

Maximum a-posterior MAP:
- p(y|x) = \integral p(y|x,\theta)... look at slides

MAP for bernouli:
- look at slides
- laplace estimation
- laplace smoothing

MAP estimation for Multi-nomial
- The conjugate prior formultinomial is called dirichlet distribution
- P(z=k) = frac{}{}

Laplace Smoothing for multinomial case:
- Look at slides

Summary:
- Generative
 - learn P(x|y) and p(y)
 - use bayes rule to compute P(y|x) for classification
 - Predict with argmaxP(y|x) or use decision theory

- Naive bayes assumes conditional independance
 - Greatly reduces the number of parameters to learn
