- Issues of over fitting:
 - it makes hypothesis space overly complex for no reason
 - over fitting makes the test data accuracy drop as training data accuracy improves

Avoid overfitting:
- Stop growing th tree when the data split does not offer large benefits
 - see if gain is significant
 - Come up with a good threshold (Not very effective. data scale changes)

- Post Pruning
 - Grow and proon back!
 - seperate training into training and validation
 - look up how to proon!

Regression Tree:
- We check the sum of square error in each branch to be smallest instead of 
  gain
- Everything else works the same

Summary:
- Decision tree:
 - Supports very complex decisions by increasing the depth of tree
 - Supports both continuous or descrete decisions
 - Learning is gready
 - Not guaranteed to find optimal decision tree
 - We might overfit noise and outliers

Ensemble leearning
- Combine many classifiers and let them vote!
- Aggregasion function combines the results and comes up with the best prediction

How to generate one:
- Main idea:
 - have the most diverese approaches
 - Bagging: Bootstrap aggregation
 - random forrest
 - Boosting
 - Stacking

Base learning Algorithm:
- black box learning algorithms

Input/s: 
Output/h: 

Bagging:
- Bootstrap aggregation
- Create many versions of the training set
- Hypothesis changes because we are using different training sets
- Use majority voting to pick the final answer

Algorithm:
1- Create T bootstrap samples {s1,..,st} of S as follows:
 - each s_i is generated by randomly drawing |S|  examples form s with replacement

2- For each i = 1,..,T h_i = learn(S_i)
3- Output H = <{h_1,...,h_t}, majorityVote>

* T needs to be tuned: Hyperparameter
* You can use any algorithm, trial and error
* There will be Duplicate S_is and S_js that are in S but not in T
* The chances are ~63% of T samples are unique

The Bias-Variance Decomposition:
- t = h(x) + nise t -> target, h -> Generative function we are trying to learn

- Expected loss = (biass)^2 + variance + noise
(Bias)^2 = \integral {\Expectation_\Di [y(x;D)] - h(x)}^2p(x) dx -> on average how well our model hits the generative function
Variance = \integral \expectation[{y(xx;D) - \expectation[y(x;D)]}^2]p(x) dx -> difference between 1 sample and average sample
noise = \integral \integrall {h(x) - t}^2p(x,t)dxdt

* P(x) is marginal ddestribution of x

Simple classifiers:
 - High bias because limited capacity
 - Low variance because they tend to be stable

Complex classifiers/model
- Low bias because they are flexible enough to capture the target
- High variance - slight change of training data leads to large change on the learned model

Bagging features:
- It cannot affect the noise! it is inherent to data, no y to modify
- It cannot change bias because bias is computed on average anyway!
- It can significantly improve variance, since variance is dependent on indovidual models and bagging averages them out

Random Forrest
- Extention to bagging
- De-corelated decisions
- use bootstrap samples to  build decision trees
 - T \subseteq S
- not look at all features
- look at subset of features
- Use gini index to select
- similar to normal decision tree but less greedy and more brute force
- Use majorityVote to get the decision
* as the number of features (members of T subset) increase, Corelation increases as well
